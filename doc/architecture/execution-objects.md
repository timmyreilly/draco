# Execution objects

**Execution objects** allow clients and extensions to efficiently and securely exchange binary objects (files).

## A simple example

To understand the need for **execution objects**, let's consider for a moment a simple extension that converts various files to PDF format. Clearly, clients need some mechanism to upload files to the extension. Conversely, the extension needs the ability to return converted files back to the client.

On the surface, this seems like a relatively simple extension to implement. Why not just exchange through an API that the client and extension share? For the simplest of scenarios that approach may make sense. But, in the real world, there are a number of challenges that make this approach less than ideal. These challenges are outlined in the following sections.

### Efficiency

Exchanging files via an API may make sense for basic, low-volume scenarios but, as the number and size of files that are being exchanged grows, performance issues become inevitable. As detailed in the [architecture overview](/doc/architecture/overview.md), clients and extensions are separated (except in the case of [direct execution](/doc/architecture/direct-execution.md)) by the execution API. If we were to exchange files directly via the execution API, the API would quickly become a performance bottleneck especially as the number executions handled by the API increases. 

To address this issue, Draco does not exchange files between clients and extensions directly. Instead, it indirectly exchanges them through [**object providers**](#object-providers). Revisiting [the PDF conversion extension](#a-simple-example) introduced earlier, the execution API uses [**object accessors**](#object-accessors) generated by [**object providers**](#object-providers) to provide the client with all the information it needs to upload the original file to a shared storage location. When the execution API calls the extension, it also uses [**object accessors**](#object-accessors) to provide the extension with all the information that it needs to both download the original file and upload the converted PDF.

By taking this approach, no additional performance overhead is placed on Draco for exchanging files between clients and extensions. Instead, Draco is merely acting as an orchestrator, ensuring that clients and extensions have all the information they need to move files back and forth directly and efficiently. More importantly, this approach gives hosts the freedom to manage the underlying storage infrastructure independently of Draco.

### Security

It's important to understand that customers, extensions, and Draco don't necessarily fully trust each other. This is especially true in multi-tenant environments where multiple customers and extensions are operating through the same hub.

Let's revisit the [example PDF conversion extension](#a-simple-example) and look at some potential security concerns.

- Exchanged files could potentially contain sensitive information such as personal identifying information (PII) and personal health information (PHI). This is of particular concern for customers that operate in highly regulated industries like finance, government, and healthcare.
- While some customers may be comfortable with their files being exchanged via cloud platforms like Azure, you may have other customers that are not and insist on files being exchanged on-premises via NFS or SMB shares.
- Some customers may be uncomfortable with their files being stored alongside those of other customers. This is especially relevant in cloud storage situations. Take, for example, [Azure blob storage](https://azure.microsoft.com/en-us/services/storage/blobs/), where files are stored in storage accounts and are automatically encrypted at rest using a key that the storage account owns. By giving each customer their own storage account, you can guarantee that files are encrypted using customer-specific keys.
- Per the [principle of least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege), clients and extensions should have only the least amount of access they need to securely exchange files. Draco adheres to this princple by providing clients and extensions with read-only and write-only **[object accessors](#object-accessors)** to only the file locations that are needed providing the right access at the right times.

By leveraging Draco's unique **[object provider](#object-providers)** model, files can be handled using completely different underlying storage infrastructure on an extension-by-extension or even customer-by-customer basis. In addition, every **[object accessor](#object-accessor)** is digitally signed using an SHA-512 hash and the hub's own private key. This approach allows you to strike a balance between cost, scalability, and security while maximizing extension reuse.

## Key concepts

![Execution objects](/doc/images/arch-execution-objects.JPG)

### Object providers

**Object providers** are essentially modular, indirect file orchestrators that faciliate exchange of files between clients and extensions. They are uniquely named and indepdently versioned.

**Object providers** are primarily responsible for generating **[object accessors](#object-accessors)** which are used by clients and extensions to exchange files. They are also responsible for organizing and maintaining the underlying file storage structure. The execution API provides **object providers** with the original execution request allowing them to efficiently partition file storage across executions, extensions, and customers.

The [key concepts](#key-concepts) diagram above features the **az-blob/v1 object provider**. This object provider orchestrates exchange of files between clients and extensions using [Azure blob storage](https://azure.microsoft.com/en-us/services/storage/blobs/). 

The **object provider** that is used for any given execution is determined by the chosen **execution profile** and the **object providers** that the extension supports. This relationship creates the flexibility needed to use different **object providers** based on customer scenario.

**Object providers** can be registered [directly through the execution API](/src/Execution.Api/Modules/Factories/ObjectAccessorProviderFactoryModule.cs) or [through the stand-alone execution agent](/src/ExecutionAdapter.ConsoleHost/Modules/ObjectAccessorProviderFactoryModule.cs).

> **Best practice**: You should consider implementing a retention policy that archives or deletes **execution objects** after a predetermined amount of time. [Azure blob storage's lifecycle management feature](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts) makes it easy to automate this.

> **Best practice**: Instead of forcing extensions to support specific **object providers**, consider creating an extension [sidecar process](https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar) that implements the needed **object providers** and stages files for extensions in a way that they can more easily access. For instance, if you're working with an extension that expects objects to be available at a specific file system path but you need to implement the **az-blobs/v1 object provider**, you could easily create a process that marshals files back and forth from Azure blob storage. This put the responsibility back on Draco to support different **object providers** while limiting changes to existing extensions.

> **Best practice**: To enable even looser coupling and frictionless, zero-downtime updates, Draco provides [a standardized mechanism for wrapping **object providers** in their own APIs](/src/ObjectStorageProvider.Api) which can then be deployed independently of the execution API and/or execution console. This also makes it easier to create **object providers** in cases where C# is not the preferred programming language.

#### Implementation

Under the hood, **object providers** implement the [IInputObjectAccessorProvider](/src/ObjectStorage/Interfaces/IInputObjectAccessorProvider.cs) and [IOutputObjectAccessorProvider](/src/ObjectStorage/Interfaces/IOutputObjectAccessorProvider.cs) interfaces. If you're creating new object providers in C#, this is the recommended approach even if you're planning on wrapping the **object provider** within its own API as highlighted in the best practice above. Using a common interface gives you greater flexibility in how you deploy **object providers**.

### Object accessors

**Object accessors** are simple, digitally-signed JSON objects that contain all the information needed to allow clients and extensions to securely exchange files. They are both scope and time-limited. **Object accessors** are generated by **[object providers](#object-providers)**.

**Object accessors** typically contain different kinds of information based on the [**object providers**](#object-providers) that generated them. For example, the **az-blobs/v1 [object provider](#object-providers)** generates read-only and write-only [shared access signatures (SAS)](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview) that allow clients and extensions to exchange files using simple HTTP GET (read-only) and PUT (write-only) requests.

Below is an example **object accessor** generated by the **az-blobs/v1 [object provider](#object-providers)**.

```json
{
  "url": "https://notarealstorageaccount.blob.core.windows.net/extensionobjects/c2afa23d-dac9-4159-8d5d-308b25ff34e5_7bb853a6-892c-4f5e-99f3-ad98e707ed93_598f9af9-f226-41b4-bd82-a2944cb1f1b1_3c8343ae-e1c7-40a3-8fe4-c60de3afbaee/input/original?sv=2019-02-02&sr=b&sig=zW5IB9yk8ynHXQ3Cl8JL0IYwRqhKmHr5boaSaLcLzUs%3D&spr=https&se=2020-01-29T02%3A01%3A05Z&sp=r",
  "urlSignature": "CwQqUSy5Qrx66SXV995FDSuX1wgFp0ccB8Ww5Qah1gF4BAy77haTs9s2G6z/8/vjZkrlxPP6IwgWGwBFcRvAVwVobukIBnP1K9HQ/ZoyvmuqYjV8to193QahpNwdPKUlw6+quKQtxuoLr+ShxNqC+3G7VhyW4yxgjjI9mOpNNjU=",
  "httpMethod": "GET",
  "accessMode": "ReadOnly",
  "expirationDateTimeUtc": "2020-01-29T02:01:05.3549743Z"
}
```

Alternatively, an SMB **[object provider](#object-providers)** may generate accessors that contain SMB file paths.
